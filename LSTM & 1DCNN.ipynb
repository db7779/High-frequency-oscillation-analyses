{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defef5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20818e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files have a column 'Unnamed: 0' with MNI brain areas and 280 to 300 more column, each corresponding to a time bin\n",
    "# Each csv corresponds to one word and has the high frequency oscillatory events (time-binned) of MNI brain areas\n",
    "# the /ENCODE directory contains csv files of words that were presented to subjects, whereas the /RECALL contains csv files of later recalled words by the subject\n",
    "\n",
    "# initialize a list with MNI brain areas by keeping the 'Unnamed: 0' column list of the first word\n",
    "df=pd.read_csv('~/ENCODE/word1.csv')\n",
    "mni_list=list(df['Unnamed: 0'])\n",
    "\n",
    "path='~/ENCODE'\n",
    "for file in os.listdir(path):\n",
    "    data=pd.read_csv(path+'/'+file)\n",
    "    data=data.set_index('Unnamed: 0')\n",
    "    if 'nan' in data.index:\n",
    "        # drop the row with NaN in the index\n",
    "        data = data.drop(index='nan')\n",
    "    if 'N/A' in data.index:\n",
    "        # drop the row with N/A in the index\n",
    "        data = data.drop(index='N/A')\n",
    "    if len(data.index)<10:\n",
    "        continue\n",
    "    else:\n",
    "        data = data.fillna(0)\n",
    "        # keep intersection\n",
    "        mni_list=list(set(mni_list) & set(list(data.index)))\n",
    "\n",
    "print(mni_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c349163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv files that are the intra-mni HFOs summed (index values are not unique)\n",
    "\n",
    "# loop over files\n",
    "for file in os.listdir(path):\n",
    "    data=pd.read_csv(path+'/'+file)\n",
    "    data=data.set_index('Unnamed: 0')\n",
    "    if 'nan' in data.index:\n",
    "        # Drop the row with NaN in the index\n",
    "        data = data.drop(index='nan')\n",
    "    if 'N/A' in data.index:\n",
    "        # Drop the row with N/A in the index\n",
    "        data = data.drop(index='N/A')\n",
    "    if len(data.index)<10:\n",
    "        continue\n",
    "    else:\n",
    "        data = data.fillna(0)\n",
    "        sum_df = data.groupby('Unnamed: 0').sum()\n",
    "        sum_df.to_csv('~/lstm/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels by assigning 0 if the word was not recalled and 1 if it was recalled\n",
    "label_dict={}\n",
    "recalled=[]\n",
    "recall_path='~/RECALL'\n",
    "for file in os.listdir(recall_path):\n",
    "    recalled.append(file.split('_')[4])\n",
    "\n",
    "for file in os.listdir('~/lstm'):\n",
    "    if file.split('_')[4] in recalled:\n",
    "        label_dict[file]=1\n",
    "    else:\n",
    "        label_dict[file]=0\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files and preprocess the data\n",
    "path = '~/lstm'\n",
    "\n",
    "X = []  # List to store input features (time series data)\n",
    "y = []  # List to store labels\n",
    "\n",
    "max_sequence_length = 150  # Set the maximum sequence length to pad the data\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.csv'):\n",
    "        #print(filename)\n",
    "        data = pd.read_csv(os.path.join(path, filename))\n",
    "        # Drop the 'Unnamed: 0' column from the DataFrame\n",
    "        data = data.drop(columns=['Unnamed: 0'])\n",
    "        # Extract time series data as input features\n",
    "        time_series = data.iloc[:, 0:max_sequence_length].values  # Adjust column range as needed\n",
    "        \n",
    "        # Check if the time_series has 27 rows\n",
    "        if time_series.shape[0] == 27:\n",
    "            X.append(time_series)\n",
    "            # Retrieve the label from the label_dict using the filename as the key\n",
    "            label = label_dict[filename]\n",
    "            y.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Pad the sequences to ensure they all have the same length\n",
    "X_padded = pad_sequences(X, maxlen=max_sequence_length, dtype='float32', padding='post')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(max_sequence_length, X_train.shape[2])))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Test the model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"\\nROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# PR-AUC Score\n",
    "pr_auc = average_precision_score(y_test, y_pred_probs)\n",
    "print(\"PR-AUC Score:\", pr_auc)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f415aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(max_sequence_length, X_train.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "#%%\n",
    "# Test the LSTM model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary values based on a threshold of 0.5\n",
    "threshold = 0.5\n",
    "binary_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"\\nROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# PR-AUC Score\n",
    "pr_auc = average_precision_score(y_test, y_pred_probs)\n",
    "print(\"PR-AUC Score:\", pr_auc)\n",
    "\n",
    "# Balanced Accuracy\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91037bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
