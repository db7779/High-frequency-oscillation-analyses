{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import approximation\n",
    "import math\n",
    "import networkx as nx\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import glob\n",
    "import ast\n",
    "import numpy as np\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from networkx import assortativity\n",
    "from networkx import distance_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for visualizing graphs (from correlation matrices) and calculating small world measures\n",
    "# functions written by Dirk Gütlin, https://digyt.github.io/graph_measures_implementation/\n",
    "\n",
    "def remove_self_connections(matrix):\n",
    "    \"\"\"Removes the self-connections of a network graph.\"\"\"\n",
    "    for i in range(len(matrix)):\n",
    "        matrix[i, i] = 0\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def invert_matrix(matrix):\n",
    "    \"\"\"Invert a matrix from 0 to 1, dependent on whether dealing with distances or weight strengths.\"\"\"\n",
    "    new_matrix = 1 - matrix.copy()\n",
    "    return remove_self_connections(new_matrix)\n",
    "\n",
    "\n",
    "def plot_graph(graph_matrix, title=\"Connectivity/Graph Matrix\", show=True, cb=True):\n",
    "    plt.pcolormesh(graph_matrix, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Channels/Nodes')\n",
    "    plt.xlabel('Channels/Nodes')\n",
    "    if cb:\n",
    "        plt.colorbar(ticks=[0, .5, 1])\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def weighted_shortest_path(matrix):\n",
    "    \"\"\"\n",
    "    Calculate the shortest path lengths between all nodes in a weighted graph.\n",
    "\n",
    "    This is an implementation of the Floyd-Warshall algorithm for finding the shortest\n",
    "    path lengths of an entire graph matrix. Implementation taken from:\n",
    "    https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\n",
    "\n",
    "    This implementation is actually identical to scipy.sparse.csgraph.floyd_warshall,\n",
    "    which we found after the implementation.\n",
    "    \"\"\"\n",
    "    matrix = invert_matrix(matrix)\n",
    "\n",
    "    n_nodes = len(matrix)\n",
    "    distances = np.empty([n_nodes, n_nodes])\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(n_nodes):\n",
    "            distances[i, j] = matrix[i, j]\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        distances[i, i] = 0\n",
    "\n",
    "    for k in range(n_nodes):\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(n_nodes):\n",
    "                if distances[i, j] > distances[i, k] + distances[k, j]:\n",
    "                    distances[i, j] = distances[i, k] + distances[k, j]\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def weighted_characteristic_path_length(matrix):\n",
    "    \"\"\"Calculate the characteristic path length for weighted graphs.\"\"\"\n",
    "    n_nodes = len(matrix)\n",
    "    min_distances = weighted_shortest_path(matrix)\n",
    "\n",
    "    sum_vector = np.empty(n_nodes)\n",
    "    for i in range(n_nodes):\n",
    "        # calculate the inner sum\n",
    "        sum_vector[i] = (1/(n_nodes-1)) * np.sum([min_distances[i, j]\n",
    "                                                  for j in range(n_nodes) if j != i])\n",
    "\n",
    "    return (1/n_nodes) * np.sum(sum_vector)\n",
    "\n",
    "\n",
    "def weighted_node_degree(matrix):\n",
    "    \"\"\"Calculate the node degree for all nodes in a weighted graph.\"\"\"\n",
    "    return np.sum(matrix, axis=-1)\n",
    "\n",
    "\n",
    "def unweighted_node_degree(matrix):\n",
    "    \"\"\"Calculate the node degree for all nodes in a weighted graph.\"\"\"\n",
    "    return np.sum(np.ceil(matrix), axis=-1)\n",
    "\n",
    "\n",
    "def weighted_triangle_number(matrix):\n",
    "    \"\"\"Calculate the weighted geometric mean of triangles around i for all nodes i in a weighted graph.\"\"\"\n",
    "    n_nodes = len(matrix)\n",
    "\n",
    "    mean_vector = np.empty([n_nodes])\n",
    "    for i in range(n_nodes):\n",
    "        triangles = np.array([[matrix[i, j] * matrix[i, k] * matrix[j, k]\n",
    "                             for j in range(n_nodes)] for k in range(n_nodes)])**(1/3)\n",
    "        mean_vector[i] = (1/2) * np.sum(triangles, axis=(0, 1))\n",
    "\n",
    "    return mean_vector\n",
    "\n",
    "\n",
    "def weighted_clustering_coeff(matrix):\n",
    "    \"\"\"Calculate the clustering coefficient for a weighted graph.\"\"\"\n",
    "    n = len(matrix)\n",
    "    t = weighted_triangle_number(matrix)\n",
    "    # here we use the !max possible weights as reference\n",
    "    k = unweighted_node_degree(matrix)\n",
    "    return (1/n) * np.sum((2 * t)/(k * (k - 1)))\n",
    "\n",
    "\n",
    "def weighted_clustering_coeff_z(matrix):\n",
    "    \"\"\"Zhang's CC is an alternative clustering coefficient which should work better for our case See Samaräki et al. (2008).\"\"\"\n",
    "    n_nodes = len(matrix)\n",
    "    ccs = []\n",
    "    for i in range(n_nodes):\n",
    "        upper = np.sum([[matrix[i, j] * matrix[j, k] * matrix[i, k]\n",
    "                       for k in range(n_nodes)] for j in range(n_nodes)])\n",
    "        lower = np.sum([[matrix[i, j] * matrix[i, k]\n",
    "                       for k in range(n_nodes) if j != k] for j in range(n_nodes)])\n",
    "        ccs.append(upper/lower)\n",
    "\n",
    "    return np.mean(ccs)\n",
    "\n",
    "def lattice_reference(G, niter=1, D=None, seed=np.random.seed(np.random.randint(0, 2**30))):\n",
    "    \"\"\"Latticize the given graph by swapping edges. Works similar to networkx' lattice reference.\"\"\"\n",
    "    from networkx.utils import cumulative_distribution, discrete_sequence\n",
    "\n",
    "    # Instead of choosing uniformly at random from a generated edge list,\n",
    "    # this algorithm chooses nonuniformly from the set of nodes with\n",
    "    # probability weighted by degree.\n",
    "    G = G.copy()\n",
    "    keys = [i for i in range(len(G))]\n",
    "    degrees = weighted_node_degree(G)\n",
    "    cdf = cumulative_distribution(degrees)  # cdf of degree\n",
    "\n",
    "    nnodes = len(G)\n",
    "    nedges = nnodes * (nnodes - 1) // 2  # NOTE: assuming full connectivity\n",
    "    if D is None:\n",
    "        D = np.zeros((nnodes, nnodes))\n",
    "        un = np.arange(1, nnodes)\n",
    "        um = np.arange(nnodes - 1, 0, -1)\n",
    "        u = np.append((0,), np.where(un < um, un, um))\n",
    "\n",
    "        for v in range(int(np.ceil(nnodes / 2))):\n",
    "            D[nnodes - v - 1, :] = np.append(u[v + 1:], u[: v + 1])\n",
    "            D[v, :] = D[nnodes - v - 1, :][::-1]\n",
    "\n",
    "    niter = niter * nedges\n",
    "    ntries = int(nnodes * nedges / (nnodes * (nnodes - 1) / 2))\n",
    "    swapcount = 0\n",
    "\n",
    "    for i in range(niter):\n",
    "        n = 0\n",
    "        while n < ntries:\n",
    "            # pick two random edges without creating edge list\n",
    "            # choose source node indices from discrete distribution\n",
    "            (ai, bi, ci, di) = discrete_sequence(\n",
    "                4, cdistribution=cdf, seed=seed)\n",
    "            if len(set((ai, bi, ci, di))) < 4:\n",
    "                continue  # picked same node twice\n",
    "            a = keys[ai]  # convert index to label\n",
    "            b = keys[bi]\n",
    "            c = keys[ci]\n",
    "            d = keys[di]\n",
    "\n",
    "            is_closer = D[ai, bi] >= D[ci, di]\n",
    "            is_larger = (G[ai, bi] >= G[ci, di])\n",
    "            if is_closer and is_larger:\n",
    "                # only swap if we get closer to the diagonal\n",
    "\n",
    "                ab = G[a, b]\n",
    "                cd = G[c, d]\n",
    "                G[a, b] = cd\n",
    "                G[b, a] = cd\n",
    "                G[c, d] = ab\n",
    "                G[d, c] = ab\n",
    "\n",
    "                swapcount += 1\n",
    "                break\n",
    "            n += 1\n",
    "    return G\n",
    "\n",
    "\n",
    "def random_reference(G, niter=1, D=None, seed=np.random.seed(np.random.randint(0, 2**30))):\n",
    "    \"\"\"Latticize the given graph by swapping edges. Works similar to networkx' random reference.\"\"\"\n",
    "    from networkx.utils import cumulative_distribution, discrete_sequence\n",
    "\n",
    "    # Instead of choosing uniformly at random from a generated edge list,\n",
    "    # this algorithm chooses nonuniformly from the set of nodes with\n",
    "    # probability weighted by degree.\n",
    "    G = G.copy()\n",
    "    keys = [i for i in range(len(G))]\n",
    "    degrees = weighted_node_degree(G)\n",
    "    cdf = cumulative_distribution(degrees)  # cdf of degree\n",
    "\n",
    "    nnodes = len(G)\n",
    "    nedges = nnodes * (nnodes - 1) // 2  # NOTE: assuming full connectivity\n",
    "    if D is None:\n",
    "        D = np.zeros((nnodes, nnodes))\n",
    "        un = np.arange(1, nnodes)\n",
    "        um = np.arange(nnodes - 1, 0, -1)\n",
    "        u = np.append((0,), np.where(un < um, un, um))\n",
    "\n",
    "        for v in range(int(np.ceil(nnodes / 2))):\n",
    "            D[nnodes - v - 1, :] = np.append(u[v + 1:], u[: v + 1])\n",
    "            D[v, :] = D[nnodes - v - 1, :][::-1]\n",
    "\n",
    "    niter = niter * nedges\n",
    "    ntries = int(nnodes * nedges / (nnodes * (nnodes - 1) / 2))\n",
    "    swapcount = 0\n",
    "\n",
    "    for i in range(niter):\n",
    "        n = 0\n",
    "        while n < ntries:\n",
    "            # pick two random edges without creating edge list\n",
    "            # choose source node indices from discrete distribution\n",
    "            (ai, bi, ci, di) = discrete_sequence(\n",
    "                4, cdistribution=cdf, seed=seed)\n",
    "            if len(set((ai, bi, ci, di))) < 4:\n",
    "                continue  # picked same node twice\n",
    "            a = keys[ai]  # convert index to label\n",
    "            b = keys[bi]\n",
    "            c = keys[ci]\n",
    "            d = keys[di]\n",
    "\n",
    "            # only swap if we get closer to the diagonal\n",
    "\n",
    "            ab = G[a, b]\n",
    "            cd = G[c, d]\n",
    "            G[a, b] = cd\n",
    "            G[b, a] = cd\n",
    "            G[c, d] = ab\n",
    "            G[d, c] = ab\n",
    "\n",
    "            swapcount += 1\n",
    "            break\n",
    "\n",
    "    return G\n",
    "\n",
    "def random_reference_nx(matrix, niter=10):\n",
    "    G = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_ref = nx.algorithms.smallworld.random_reference(G, niter=niter)\n",
    "    return nx.convert_matrix.to_numpy_array(G_ref)\n",
    "\n",
    "\n",
    "def lattice_reference_nx(matrix, niter=10):\n",
    "    G = nx.convert_matrix.from_numpy_array(matrix)\n",
    "    G_ref = nx.algorithms.smallworld.lattice_reference(G, niter=niter)\n",
    "    return nx.convert_matrix.to_numpy_array(G_ref)\n",
    "\n",
    "\n",
    "def weighted_sw_sigma(matrix, n_avg=1):\n",
    "    \"\"\"Calculate the weighted small world coefficient sigma of a matrix.\"\"\"\n",
    "    sigmas = []\n",
    "    for i in range(n_avg):\n",
    "        random_graph = random_reference(matrix)\n",
    "        C = weighted_clustering_coeff_z(matrix)\n",
    "        C_rand = weighted_clustering_coeff_z(random_graph)\n",
    "        L = weighted_characteristic_path_length(matrix)\n",
    "        L_rand = weighted_characteristic_path_length(random_graph)\n",
    "        sigma = (C/C_rand) / (L/L_rand)\n",
    "        sigmas.append(sigma)\n",
    "\n",
    "    return np.mean(sigmas)\n",
    "\n",
    "\n",
    "def weighted_sw_omega(matrix, n_avg=1):\n",
    "    \"\"\"Calculate the weighted small world coefficient omega of a matrix.\"\"\"\n",
    "    omegas = []\n",
    "    for i in range(n_avg):\n",
    "        random_graph = random_reference(matrix)\n",
    "        lattice_graph = lattice_reference(matrix)\n",
    "        C = weighted_clustering_coeff_z(matrix)\n",
    "        C_latt = weighted_clustering_coeff_z(lattice_graph)\n",
    "        L = weighted_characteristic_path_length(matrix)\n",
    "        L_rand = weighted_characteristic_path_length(random_graph)\n",
    "        omega = (L_rand/L) / (C/C_latt)\n",
    "        omegas.append(omega)\n",
    "\n",
    "    return np.mean(omegas)\n",
    "\n",
    "\n",
    "def weighted_sw_index(matrix, n_avg=1):\n",
    "    \"\"\"Calculate the weighted small world coefficient omega of a matrix.\"\"\"\n",
    "    indices = []\n",
    "    for i in range(n_avg):\n",
    "        random_graph = random_reference(matrix)\n",
    "        lattice_graph = lattice_reference(matrix)\n",
    "        C = weighted_clustering_coeff_z(matrix)\n",
    "        C_rand = weighted_clustering_coeff_z(random_graph)\n",
    "        C_latt = weighted_clustering_coeff_z(lattice_graph)\n",
    "        L = weighted_characteristic_path_length(matrix)\n",
    "        L_rand = weighted_characteristic_path_length(random_graph)\n",
    "        L_latt = weighted_characteristic_path_length(lattice_graph)\n",
    "        index = ((L - L_latt) / (L_rand - L_latt)) * \\\n",
    "            ((C - C_rand) / (C_latt - C_rand))\n",
    "        indices.append(index)\n",
    "    return np.mean(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted graph measures from correlation matrices (each matrix corresponding to a word, MNI x MNI)\n",
    "PATH='~/corr_matrices'\n",
    "# Create a dataframe that will store the graph measures for all CSVs\n",
    "graph_df_abs=pd.DataFrame(columns=['weighted_characteristic_path_length','weighted_clustering_coeff_z',\n",
    "                                                                 'weighted_sw_sigma','average_clustering','diameter','radius',\n",
    "                                                                 'transitivity','degree_assortativity_coefficient','global_efficiency',\n",
    "                                                                 'local_efficiency'])\n",
    "for condition in os.listdir(PATH):\n",
    "    for csv in os.listdir(PATH+'/'+condition):\n",
    "        #print(condition,csv)\n",
    "        data = pd.read_csv(PATH+'/'+condition+'/'+csv)\n",
    "        data.set_index('Unnamed: 0',inplace=True)\n",
    "        # drop empty rows\n",
    "        data = data.dropna(how='all')\n",
    "        # drop empty columns\n",
    "        data = data.dropna(axis=1, how='all')\n",
    "        # Fill empty values with zeros\n",
    "        data.fillna(0, inplace=True)\n",
    "        # Create weighted graph - convert to absolute values\n",
    "        matrix = data.abs().values\n",
    "        name=condition+'_'+csv\n",
    "        graph_df_abs.loc[name,'weighted_characteristic_path_length']=weighted_characteristic_path_length(matrix)\n",
    "        graph_df_abs.loc[name,'weighted_clustering_coeff_z']=weighted_clustering_coeff_z(matrix)\n",
    "        graph_df_abs.loc[name,'weighted_sw_sigma']=weighted_sw_sigma(matrix)\n",
    "        # Create distance graph before applying built-in networkx functions\n",
    "        G_gt=nx.from_numpy_array(1-matrix)\n",
    "        # Remove self loops in the graph\n",
    "        G_gt.remove_edges_from(nx.selfloop_edges(G_gt))\n",
    "        graph_df_abs.loc[name,'average_clustering']=nx.average_clustering(G_gt,weight='weight')\n",
    "        graph_df_abs.loc[name,'diameter']=nx.diameter(G_gt,weight='weight')\n",
    "        graph_df_abs.loc[name,'radius']=nx.radius(G_gt,weight='weight')\n",
    "        graph_df_abs.loc[name,'transitivity']=nx.transitivity(G_gt)\n",
    "        graph_df_abs.loc[name,'degree_assortativity_coefficient']=nx.degree_assortativity_coefficient(G_gt,weight='weight')\n",
    "        graph_df_abs.loc[name,'global_efficiency']=nx.global_efficiency(G_gt)\n",
    "        graph_df_abs.loc[name,'local_efficiency']=nx.local_efficiency(G_gt)\n",
    "\n",
    "# save the dataframe as a csv\n",
    "graph_df_abs.to_csv('~/graph_measures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceee026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
